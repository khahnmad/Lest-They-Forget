{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141b5608-488b-42af-8f4b-6203de06b92d",
   "metadata": {},
   "source": [
    "# To Validate tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a080831c-184a-4e64-918e-589525f800ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "random.seed(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162077b6-c935-4643-aac8-26a1203936a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addCol_takeSample(df):\n",
    "    '''\n",
    "    parameter: DataFrame\n",
    "    Returns: Two dataframes:\n",
    "                        - with two additional columns, verified_tweet: if not Nan, it belongs to the gold standard subset.\n",
    "                                                       mentioned_shooting: if not Nan, the article corresponds to another shooting.\n",
    "                        - It takes 2% of the articles as sample to be be verified. \n",
    "                                                       if the 2% is less than 20 tweets, it either returns all the tweets or 20 tweets.\n",
    "                                                                            \n",
    "    '''\n",
    "    df['verified_tweet'] = np.nan\n",
    "    df['mentioned_shooting'] = np.nan\n",
    "  \n",
    "    sample = df.sample(frac=0.01, random_state=33)\n",
    "        \n",
    "    return df, sample\n",
    "\n",
    "\n",
    "\n",
    "def validate_tweets(df, df_sample, label):\n",
    "\n",
    "    df_update = copy.deepcopy(df)\n",
    "    df_sample_update = copy.deepcopy(df_sample)\n",
    "    j=0\n",
    "    for i, row in df_sample.iterrows():\n",
    "        j+=1\n",
    "        print(j, label)\n",
    "        print(row.date)\n",
    "        print(row.tweet)\n",
    "#         print(row['0'])\n",
    "        reply = input()\n",
    "        \n",
    "        if reply == 'j': \n",
    "            print(' The tweet IS related to this shooting.')\n",
    "            df_update.at[i, ['verified_tweet']] = 'related'\n",
    "            df_sample_update.at[i, ['verified_tweet']] = 'related'\n",
    "            print('-----')\n",
    "#             also_related = input()\n",
    "#             if also_related == 'aj':\n",
    "#                 about = input()\n",
    "#                 print(f'The tweet is about the shooting : {about}')\n",
    "#                 df_update.at[i, ['mentioned_shooting']] = f'{about}'\n",
    "#                 df_sample_update.at[i, ['mentioned_shooting']] = f'{about}'\n",
    "#                 print('----------')\n",
    "                \n",
    "        if reply == 'm':\n",
    "            print('The tweet IS NOT related to the shooting.')\n",
    "            print('----------')\n",
    "            \n",
    "            about = input()\n",
    "            print(f'The tweet IS about the shooting : {about}')\n",
    "            df_update.at[i, ['verified_tweet']] = 'not-related'\n",
    "            df_sample_update.at[i, ['verified_tweet']] = 'not-related'\n",
    "            df_update.at[i, ['mentioned_shooting']] = f'{about}'\n",
    "            df_sample_update.at[i, ['mentioned_shooting']] = f'{about}'\n",
    "            print('----------')\n",
    "            \n",
    "        if reply == 'k':\n",
    "            print('The tweet IS NOT related to the shooting, no other shooting mentioned')\n",
    "            df_update.at[i, ['verified_tweet']] = 'not-related'\n",
    "            df_sample_update.at[i, ['verified_tweet']] = 'not-related'\n",
    "            print('----------')\n",
    "        \n",
    "    return df_update, df_sample_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c0b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv_path in glob.glob('./Dict_*.csv'):\n",
    "    df_updated_path = './Dict_updated_csv/' + csv_path[2:-4] + '_updated.csv'\n",
    "    df_sample_updated_path = './Dict_updated_csv/' + csv_path[2:-4] + '_sample.csv'\n",
    "    \n",
    "    if Path(df_updated_path).is_file():\n",
    "        continue\n",
    "\n",
    "\n",
    "    try:\n",
    "        csv = pd.read_csv(csv_path)\n",
    "    except:\n",
    "        print(csv_path)\n",
    "        continue\n",
    "    df, df_sample = addCol_takeSample(csv)\n",
    "    \n",
    "    df_updated, df_sample_updated = validate_tweets(df, df_sample, csv_path[2:-4])\n",
    "\n",
    "    df_updated.to_csv(df_updated_path, sep=',', encoding='utf-8', index=False)\n",
    "    df_sample_updated.to_csv(df_sample_updated_path, sep=',', encoding='utf-8', index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2dddd0-7791-402b-8d67-e32c495d3e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = general_phrases = [\n",
    "    \"shooting\", \"tragedy\", \"condolences\", \"thoughts\", \"prayers\", \"thoughts prayers\",\"murder\", \"killing\", \"shooter\", \"shooters\",\n",
    "    \"armed gunman\", \"deepest condolences\", \"victims\", \"killed\", \"injured\", \"families\", \"heart\", \"shot\",\n",
    "    \"tragic\", \"enoughisenough\", \"guncontrol\", \"gunviolence\", \"guncontrol\", \"mass murder\", 'grieve', 'gun control', 'heart broken', '2a'\n",
    "]\n",
    "\n",
    "dictionary = {\n",
    "    'Plano':[\"spencer hight\",\"meredith hight\",\"plano\",\"caleb edwards\",\"deffner\",\"rushin\",'hight','estranged husband', 'north texas'],\n",
    "    'Pittsburgh':[\"pittsburgh\",\"synagogue\",\"bowers\",\"tree life\",\"squirrel hill\", \"anti-semitism\",'pittsburghsynagogue','treeoflife','treeoflifesynagogue','showupforshabbat', 'pittsburghshooting', 'pittsburghstrong'],\n",
    "    'Las_Vegas':[\"paddock\", \"vegas\",\"las vegas\",\"lombardo\",'country music event','music festival','vegasstrong', '1Ooctober', 'vegasstrong','lasvegasstrong','lasvegasshooting', 'prayforvegas'],\n",
    "    'SanBernardino':[\"san bernardino\",'bernadino', 'sbstrong', 'prayforsanbernardino', 'sanbernardino'],\n",
    "    'Houston':['harris county', 'gilliland', 'valerie jackson', 'dewayne jackson', 'david ray conley', 'conley'],\n",
    "    'Odessa':['midland','odessa', 'midland-odessa','west texas','midland-odessa', 'odessastrong', 'odessashooting', 'odessastrong', 'westtexasstrong'],\n",
    "    'Bogue_Chitto':['bogue chitto','lincoln county','brookhaven', 'godbolt', 'durr'],\n",
    "    'Washington':['washington navy yard', 'navy yard', 'navyyard', 'navyyardshooting'],\n",
    "    'Boulder':['boulder','grocery store', 'arvada', 'king soopers', 'boulderstrong', 'boulderproud'],\n",
    "    'Virginia':['virginia beach','nettleton','princess anne', 'municipal center', 'virginiabeach', 'cityofvabeach', 'vbremembers', 'vbstrong', 'loveforvb']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6713b1d8-1fbe-4a3e-a1df-dc84098a1fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_check(text, keywords, general_keywords):\n",
    "    num_of_k_occur = sum([1 if k in text else 0 for k in keywords])\n",
    "    num_of_gk_occur = sum([1 if k in text else 0 for k in general_keywords])\n",
    "    return num_of_k_occur + num_of_gk_occur\n",
    "\n",
    "def gen_csv_path(label, prefix='.'):\n",
    "    return f\"{prefix}/Twitter_{label}_unfiltered.csv\"\n",
    "\n",
    "def gen_new_csv_path(label, prefix='./Filtered_tweets_csv/'):\n",
    "    return f\"{prefix}/Twitter_{label}_filtered.csv\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e903c-6764-408c-b5f8-ef0151b1d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label, keywords in dictionary.items():\n",
    "#     csv_path = gen_csv_path(label)\n",
    "    \n",
    "#     csv = pd.read_csv(csv_path)\n",
    "#     csv['dict_method_n_occurance'] = 0\n",
    "    \n",
    "#     for i, tweet in csv.iterrows():\n",
    "#         csv.at[i, ['dict_method_n_occurance']] = dict_check(tweet.tweet, keywords, general_phrases)\n",
    "    \n",
    "#     new_csv = csv.loc[csv['dict_method_n_occurance'] > 0].reset_index()\n",
    "    \n",
    "#     print(f'Saving {label} csv file...')\n",
    "#     new_csv_path = gen_new_csv_path(label)\n",
    "#     new_csv.to_csv(new_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd95a155-760b-4bf8-b276-e550e8960fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for csv_path in glob.glob('./Filtered_tweets_csv/Twitter_*.csv'):\n",
    "#     df_updated_path = './Filtered_tweets_csv/validated_tweets/' + csv_path[22:-4] + '_updated.csv'\n",
    "#     df_sample_updated_path = './Filtered_tweets_csv/validated_tweets/' + csv_path[22:-4] + '_sample.csv'\n",
    "    \n",
    "#     if Path(df_updated_path).is_file():\n",
    "#         continue\n",
    "        \n",
    "#     csv = pd.read_csv(csv_path)\n",
    "\n",
    "#     df, df_sample = addCol_takeSample(csv)\n",
    "    \n",
    "#     df_updated, df_sample_updated = validate_tweets(df, df_sample, csv_path[22:-4])\n",
    "\n",
    "#     df_updated.to_csv(df_updated_path, sep=',', encoding='utf-8', index=False)\n",
    "#     df_sample_updated.to_csv(df_sample_updated_path, sep=',', encoding='utf-8', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
