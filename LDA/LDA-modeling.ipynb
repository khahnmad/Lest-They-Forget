{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import glob\n",
    "import All_Functions as af\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the version 2 text files\n",
    "all_files = [x for x in glob.glob('newspaper-text' + \"/*.csv\") if \"v2\" in x]\n",
    "all_text = af.import_text_data(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n"
     ]
    }
   ],
   "source": [
    "# Put all text into a single df \n",
    "compact_list =[all_text[0][0]]\n",
    "for dataset in all_text: # THERES a memeory problem with using the full dataset at the doc term matrix step!!!!\n",
    "    for i in range(1,len(dataset)):\n",
    "        compact_list.append(dataset[i])\n",
    "        \n",
    "sample = random.sample(range(1, len(compact_list)), 7000)\n",
    "\n",
    "sample_list = [compact_list[i] for i in range(len(compact_list)) if i in sample]\n",
    "print(len(sample_list))\n",
    "\n",
    "df = pd.DataFrame(data=sample_list, columns=compact_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use CountVectorizer to create a document-term matrix \n",
    "The paramters mean that we're only including words that appear in less than 80% of the document and in at least 2 documents\n",
    "Also removing stopwords\n",
    "\"\"\"\n",
    "count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "doc_term_matrix = count_vect.fit_transform(df['text'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7000x43241 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2071950 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This means that each of the 20000 documents is represented as a vector with \n",
    "1456 dimension - so our vocabulary has 14546 words'''\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=4, random_state=42)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "n_components specifics the number of topics that we want our text to be divided into\n",
    "random_state is the seed so you can replicate your results \n",
    "'''\n",
    "LDA = LatentDirichletAllocation(n_components=4, random_state=42)\n",
    "LDA.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['photo', 'said', 'sunday', 'music', 'paddock', 'festival', '2017', 'people', 'las', 'vegas']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['time', 'trump', 'year', 'going', 'state', 'said', 'new', 'like', 'gun', 'people']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['white', 'time', 'texas', 'violence', 'news', 'president', 'people', 'trump', 'gun', 'said']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['time', 'news', 'ufc', 'las', 'school', 'told', 'vegas', 'police', 'family', 'said']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7000, 6)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_values = LDA.transform(doc_term_matrix)\n",
    "'''output means that each of the documents have 5 columns where each column \n",
    "corresponds to the probabilitiy value of a particular topic'''\n",
    "topic_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>title</th>\n",
       "      <th>themes</th>\n",
       "      <th>media_id</th>\n",
       "      <th>media_url</th>\n",
       "      <th>Leaning</th>\n",
       "      <th>Location</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>http://feedproxy.google.com/~r/time/topstories...</td>\n",
       "      <td>least five people killed monday disgruntled em...</td>\n",
       "      <td>{'neg': 0.224, 'neu': 0.74, 'pos': 0.036, 'com...</td>\n",
       "      <td>2017-06-05 11:46:24</td>\n",
       "      <td>‘Disgruntled’ Ex-Employee Kills 5 in Orlando S...</td>\n",
       "      <td></td>\n",
       "      <td>40362</td>\n",
       "      <td>http://www.time.com/time/</td>\n",
       "      <td>Bogue</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>http://www.fark.com/comments/9617590?utm_sourc...</td>\n",
       "      <td>iczer ca wait see twit chief say ... marcus au...</td>\n",
       "      <td>{'neg': 0.192, 'neu': 0.602, 'pos': 0.206, 'co...</td>\n",
       "      <td>2017-06-07 03:33:23</td>\n",
       "      <td>Mass shooting reported (spins wheel) inside Ir...</td>\n",
       "      <td></td>\n",
       "      <td>19921</td>\n",
       "      <td>http://www.fark.com</td>\n",
       "      <td>Bogue</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>http://feeds.chicagotribune.com/~r/chicagotrib...</td>\n",
       "      <td>family members crowded small corridor emergenc...</td>\n",
       "      <td>{'neg': 0.161, 'neu': 0.828, 'pos': 0.011, 'co...</td>\n",
       "      <td>2017-06-11 11:34:00</td>\n",
       "      <td>2 injured on Riverwalk and 9 in Lawndale among...</td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>http://www.chicagotribune.com/</td>\n",
       "      <td>center</td>\n",
       "      <td>Bogue</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>http://www.startribune.com/local/427850713.html</td>\n",
       "      <td>rainbows dotted metro area sunday despite gray...</td>\n",
       "      <td>{'neg': 0.079, 'neu': 0.77, 'pos': 0.152, 'com...</td>\n",
       "      <td>2017-06-11 22:19:34</td>\n",
       "      <td>Pride rally, festival in Minn. carry theme of ...</td>\n",
       "      <td></td>\n",
       "      <td>19</td>\n",
       "      <td>http://www.startribune.com/</td>\n",
       "      <td>Bogue</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17</td>\n",
       "      <td>http://network.yardbarker.com/nba/article_exte...</td>\n",
       "      <td>free newsletters facebook twitter flipboard yo...</td>\n",
       "      <td>{'neg': 0.028, 'neu': 0.818, 'pos': 0.154, 'co...</td>\n",
       "      <td>2017-06-12 01:23:50</td>\n",
       "      <td>Orlando found its identity in the year after P...</td>\n",
       "      <td></td>\n",
       "      <td>86839</td>\n",
       "      <td>http://www.yardbarker.com/#spider</td>\n",
       "      <td>center</td>\n",
       "      <td>Bogue</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url  \\\n",
       "0   3  http://feedproxy.google.com/~r/time/topstories...   \n",
       "1   6  http://www.fark.com/comments/9617590?utm_sourc...   \n",
       "2  10  http://feeds.chicagotribune.com/~r/chicagotrib...   \n",
       "3  16    http://www.startribune.com/local/427850713.html   \n",
       "4  17  http://network.yardbarker.com/nba/article_exte...   \n",
       "\n",
       "                                                text  \\\n",
       "0  least five people killed monday disgruntled em...   \n",
       "1  iczer ca wait see twit chief say ... marcus au...   \n",
       "2  family members crowded small corridor emergenc...   \n",
       "3  rainbows dotted metro area sunday despite gray...   \n",
       "4  free newsletters facebook twitter flipboard yo...   \n",
       "\n",
       "                                           sentiment         publish_date  \\\n",
       "0  {'neg': 0.224, 'neu': 0.74, 'pos': 0.036, 'com...  2017-06-05 11:46:24   \n",
       "1  {'neg': 0.192, 'neu': 0.602, 'pos': 0.206, 'co...  2017-06-07 03:33:23   \n",
       "2  {'neg': 0.161, 'neu': 0.828, 'pos': 0.011, 'co...  2017-06-11 11:34:00   \n",
       "3  {'neg': 0.079, 'neu': 0.77, 'pos': 0.152, 'com...  2017-06-11 22:19:34   \n",
       "4  {'neg': 0.028, 'neu': 0.818, 'pos': 0.154, 'co...  2017-06-12 01:23:50   \n",
       "\n",
       "                                               title themes media_id  \\\n",
       "0  ‘Disgruntled’ Ex-Employee Kills 5 in Orlando S...           40362   \n",
       "1  Mass shooting reported (spins wheel) inside Ir...           19921   \n",
       "2  2 injured on Riverwalk and 9 in Lawndale among...               9   \n",
       "3  Pride rally, festival in Minn. carry theme of ...              19   \n",
       "4  Orlando found its identity in the year after P...           86839   \n",
       "\n",
       "                           media_url Leaning Location  Topic  \n",
       "0          http://www.time.com/time/   Bogue     None      4  \n",
       "1                http://www.fark.com   Bogue     None      2  \n",
       "2     http://www.chicagotribune.com/  center    Bogue      4  \n",
       "3        http://www.startribune.com/   Bogue     None      2  \n",
       "4  http://www.yardbarker.com/#spider  center    Bogue      5  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Topic'] = topic_values.argmax(axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LatentDirichletAllocation' object has no attribute 'id2word'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-a053f057facb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCoherenceModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Compute Coherence Score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mcoherence_model_lda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCoherenceModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLDA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoc_term_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoherence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c_v'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mcoherence_lda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoherence_model_lda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_coherence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nCoherence Score: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoherence_lda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\coherencemodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, model, topics, texts, corpus, dictionary, window_size, keyed_vectors, coherence, topn, processes)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[1;31m# Check if associated dictionary is provided.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFakeDict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m                 raise ValueError(\n\u001b[0;32m    177\u001b[0m                     \u001b[1;34m\"The associated dictionary should be provided with the corpus or 'id2word'\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LatentDirichletAllocation' object has no attribute 'id2word'"
     ]
    }
   ],
   "source": [
    "# source: https://towardsdatascience.com/evaluate-topic-model-in-python-latent-dirichlet-allocation-lda-7d57484bb5d0\n",
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=LDA, texts=doc_term_matrix, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# same source4\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(sample_list)\n",
    "# Create Corpus\n",
    "texts = sample_list\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1])\n",
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=sample_list, dictionary=id2word, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/540 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Validation_Set': [], 'Topics': [], 'Alpha': [], 'Beta': [], 'Coherence': []}\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 30/540 [16:55<4:59:32, 35.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 60/540 [34:14<3:43:50, 27.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 90/540 [47:17<3:19:00, 26.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/540 [3:06:41<164:54:44, 1120.16s/it]\n",
      " 22%|██▏       | 120/540 [1:01:23<3:15:33, 27.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 150/540 [1:16:42<3:45:57, 34.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 180/540 [1:37:55<4:07:30, 41.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 210/540 [1:56:54<2:37:37, 28.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 240/540 [2:14:59<3:13:57, 38.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 270/540 [2:34:42<2:56:42, 39.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 300/540 [5:30:38<17:51:25, 267.86s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 330/540 [5:45:26<1:38:18, 28.09s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 360/540 [5:58:13<1:22:54, 27.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 390/540 [6:14:50<1:16:04, 30.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 420/540 [6:31:11<52:16, 26.14s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 450/540 [6:45:04<42:12, 28.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 480/540 [6:59:13<27:58, 27.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 510/540 [7:15:43<16:30, 33.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 540/540 [7:31:15<00:00, 50.14s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [# gensim.utils.ClippedCorpus(corpus, num_of_docs*0.25), \n",
    "               # gensim.utils.ClippedCorpus(corpus, num_of_docs*0.5), \n",
    "               gensim.utils.ClippedCorpus(corpus, num_of_docs*0.75), \n",
    "               corpus]\n",
    "corpus_title = ['75% Corpus', '100% Corpus']\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "print(model_results)\n",
    "\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=540)\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            print(k)\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "#                     cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=id2word, \n",
    "#                                                   k=k, a=a, b=b)\n",
    "                    cv = compute_coherence_values(corpus=corpus, dictionary=id2word, k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = {'Topics':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results['Topics'].append('helo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Topics': ['helo']}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Validation_Set</th>\n",
       "      <th>Topics</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Coherence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75% Corpus</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.522850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75% Corpus</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.375227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>75% Corpus</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.371268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>75% Corpus</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9099999999999999</td>\n",
       "      <td>0.372665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75% Corpus</td>\n",
       "      <td>2</td>\n",
       "      <td>0.01</td>\n",
       "      <td>symmetric</td>\n",
       "      <td>0.376639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Validation_Set  Topics Alpha                Beta  Coherence\n",
       "0     75% Corpus       2  0.01                0.01   0.522850\n",
       "1     75% Corpus       2  0.01                0.31   0.375227\n",
       "2     75% Corpus       2  0.01                0.61   0.371268\n",
       "3     75% Corpus       2  0.01  0.9099999999999999   0.372665\n",
       "4     75% Corpus       2  0.01           symmetric   0.376639"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('lda_tuning_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Validation_Set</th>\n",
       "      <th>Topics</th>\n",
       "      <th>Alpha</th>\n",
       "      <th>Beta</th>\n",
       "      <th>Coherence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>75% Corpus</td>\n",
       "      <td>4</td>\n",
       "      <td>asymmetric</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.572947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Validation_Set  Topics       Alpha  Beta  Coherence\n",
       "85     75% Corpus       4  asymmetric  0.01   0.572947"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Coherence'] ==df['Coherence'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.corpora as corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the version 2 text files\n",
    "all_files = [x for x in glob.glob('newspaper-text' + \"/*.csv\") if \"v2\" in x]\n",
    "all_text = af.import_text_data(all_files)\n",
    "\n",
    "# Put all text into a single df \n",
    "compact_list =[all_text[0][0]]\n",
    "for dataset in all_text: # THERES a memeory problem with using the full dataset at the doc term matrix step!!!!\n",
    "    for i in range(1,len(dataset)):\n",
    "        compact_list.append(dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n"
     ]
    }
   ],
   "source": [
    "sample = random.sample(range(1, len(compact_list)), 7000)\n",
    "\n",
    "sample_list = [compact_list[i] for i in range(len(compact_list)) if i in sample]\n",
    "print(len(sample_list))\n",
    "\n",
    "df = pd.DataFrame(data=sample_list, columns=compact_list[0])\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(sample_list)\n",
    "# Create Corpus\n",
    "texts = sample_list\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=7, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='asymmetric',\n",
    "                                           eta=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-92fbba479eb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: joblib in c:\\users\\khahn\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: setuptools in c:\\users\\khahn\\anaconda3\\lib\\site-packages (from pyLDAvis) (52.0.0.post20210125)\n",
      "Requirement already satisfied: future in c:\\users\\khahn\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\khahn\\anaconda3\\lib\\site-packages (from pyLDAvis) (0.23.1)\n",
      "Collecting funcy\n",
      "  Downloading funcy-1.16-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: gensim in c:\\users\\khahn\\anaconda3\\lib\\site-packages (from pyLDAvis) (4.0.1)\n",
      "Requirement already satisfied: numexpr in c:\\users\\khahn\\anaconda3\\lib\\site-packages (from pyLDAvis) (2.7.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\khahn\\anaconda3\\lib\\site-packages (from pyLDAvis) (1.5.0)\n",
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\khahn\\anaconda3\\lib\\site-packages (from pyLDAvis) (3.0.0)\n",
      "Collecting pandas>=1.2.0\n",
      "  Downloading pandas-1.3.2-cp38-cp38-win_amd64.whl (10.2 MB)\n",
      "Collecting numpy>=1.20.0\n",
      "  Downloading numpy-1.21.2-cp38-cp38-win_amd64.whl (14.0 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\khahn\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\khahn\\anaconda3\\lib\\site-packages (from pandas>=1.2.0->pyLDAvis) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\khahn\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: Cython==0.29.21 in c:\\users\\khahn\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (0.29.21)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\khahn\\anaconda3\\lib\\site-packages (from gensim->pyLDAvis) (3.0.0)\n",
      "Requirement already satisfied: requests in c:\\users\\khahn\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (2.25.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0.0rc2 in c:\\users\\khahn\\anaconda3\\lib\\site-packages (from jinja2->pyLDAvis) (2.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\khahn\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim->pyLDAvis) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\khahn\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim->pyLDAvis) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\khahn\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim->pyLDAvis) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\khahn\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim->pyLDAvis) (4.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\khahn\\anaconda3\\lib\\site-packages (from scikit-learn->pyLDAvis) (2.1.0)\n",
      "Building wheels for collected packages: pyLDAvis, sklearn\n",
      "  Building wheel for pyLDAvis (PEP 517): started\n",
      "  Building wheel for pyLDAvis (PEP 517): finished with status 'done'\n",
      "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136897 sha256=beb3420486b6eda11285d4df910de3a70c9c4c5c6ebc0d844241917ffb94e0b0\n",
      "  Stored in directory: c:\\users\\khahn\\appdata\\local\\pip\\cache\\wheels\\90\\61\\ec\\9dbe9efc3acf9c4e37ba70fbbcc3f3a0ebd121060aa593181a\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1316 sha256=66ff71e0c10c402f3db01f48d22ffcea2834f005222279f5924533928715ed63\n",
      "  Stored in directory: c:\\users\\khahn\\appdata\\local\\pip\\cache\\wheels\\22\\0b\\40\\fd3f795caaa1fb4c6cb738bc1f56100be1e57da95849bfc897\n",
      "Successfully built pyLDAvis sklearn\n",
      "Installing collected packages: numpy, sklearn, pandas, funcy, pyLDAvis\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.5\n",
      "    Uninstalling numpy-1.18.5:\n",
      "      Successfully uninstalled numpy-1.18.5\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\khahn\\\\anaconda3\\\\Lib\\\\site-packages\\\\~-mpy\\\\.libs\\\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the version 2 text files\n",
    "all_files = [x for x in glob.glob('relevancy-files' + \"/*.csv\")]\n",
    "# all_text = af.import_text_data(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LDA(file):\n",
    "    df = pd.read_csv(file)\n",
    "    relevant_df = df[df['Relevancy']==1]\n",
    "    print(len(relevant_df))\n",
    "    count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "    doc_term_matrix = count_vect.fit_transform(relevant_df['text'].values.astype('U'))\n",
    "    LDA = LatentDirichletAllocation(n_components=4, random_state=42)\n",
    "    LDA.fit(doc_term_matrix)\n",
    "\n",
    "    for i, topic in enumerate(LDA.components_):\n",
    "        print(f'Top 10 words for topic #{i+1}:')\n",
    "        print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "        print('\\n')\n",
    "\n",
    "    topic_values = LDA.transform(doc_term_matrix)\n",
    "    relevant_df['Topic'] = topic_values.argmax(axis=1)\n",
    "\n",
    "    pyLDAvis.sklearn.prepare(LDA, doc_term_matrix, count_vect)\n",
    "    return df.values.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boulder = run_LDA(all_files[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
