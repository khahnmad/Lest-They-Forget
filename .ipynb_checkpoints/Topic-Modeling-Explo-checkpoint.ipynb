{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA for topic modeling\n",
    "# https://stackabuse.com/python-for-nlp-topic-modeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import All_Functions as af\n",
    "\n",
    "reviews_datasets = pd.read_csv('Reviews.csv')\n",
    "reviews_datasets = reviews_datasets.head(20000)\n",
    "reviews_datasets.dropna()\n",
    "reviews_datasets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>title</th>\n",
       "      <th>themes</th>\n",
       "      <th>media_id</th>\n",
       "      <th>media_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.realclearpolitics.com/articles/201...</td>\n",
       "      <td>las vegas ap rapid-fire popping sounded like f...</td>\n",
       "      <td>{'neg': 0.212, 'neu': 0.727, 'pos': 0.061, 'co...</td>\n",
       "      <td>2017-10-01 20:00:00</td>\n",
       "      <td>At Least 50 Killed as Gunman Opens Fire at Las...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1040</td>\n",
       "      <td>http://realclearpolitics.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>http://www.marketwatch.com/news/story.asp?guid...</td>\n",
       "      <td>shares gun makers rallied monday wake describe...</td>\n",
       "      <td>{'neg': 0.117, 'neu': 0.749, 'pos': 0.133, 'co...</td>\n",
       "      <td>2017-10-02 08:35:09</td>\n",
       "      <td>Gun-maker stocks surge after mass shooting in ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1150</td>\n",
       "      <td>https://www.wsj.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>http://feedproxy.google.com/~r/newsy-allvideos...</td>\n",
       "      <td>least 58 people dead 500 injured gunman opened...</td>\n",
       "      <td>{'neg': 0.284, 'neu': 0.659, 'pos': 0.057, 'co...</td>\n",
       "      <td>2017-10-02 07:47:00</td>\n",
       "      <td>At Least 50 Dead, 400 Injured After Las Vegas ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85364</td>\n",
       "      <td>http://www.newsy.com/#spider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>http://feedproxy.google.com/~r/time/topstories...</td>\n",
       "      <td>information spreads quickly mass shootings peo...</td>\n",
       "      <td>{'neg': 0.255, 'neu': 0.687, 'pos': 0.059, 'co...</td>\n",
       "      <td>2017-10-02 10:51:55</td>\n",
       "      <td>Beware of These Hoaxes Being Spread About the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40362</td>\n",
       "      <td>http://www.time.com/time/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>http://www.eastbaytimes.com/2017/10/02/watch-l...</td>\n",
       "      <td>trending president donald trump spoke morning ...</td>\n",
       "      <td>{'neg': 0.154, 'neu': 0.789, 'pos': 0.057, 'co...</td>\n",
       "      <td>2017-10-02 10:35:10</td>\n",
       "      <td>Watch: President Trump speaks about Las Vegas ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27512</td>\n",
       "      <td>http://www.ibabuzz.com/insider/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                url  \\\n",
       "0           0  https://www.realclearpolitics.com/articles/201...   \n",
       "1           1  http://www.marketwatch.com/news/story.asp?guid...   \n",
       "2           2  http://feedproxy.google.com/~r/newsy-allvideos...   \n",
       "3           3  http://feedproxy.google.com/~r/time/topstories...   \n",
       "4           4  http://www.eastbaytimes.com/2017/10/02/watch-l...   \n",
       "\n",
       "                                                text  \\\n",
       "0  las vegas ap rapid-fire popping sounded like f...   \n",
       "1  shares gun makers rallied monday wake describe...   \n",
       "2  least 58 people dead 500 injured gunman opened...   \n",
       "3  information spreads quickly mass shootings peo...   \n",
       "4  trending president donald trump spoke morning ...   \n",
       "\n",
       "                                           sentiment         publish_date  \\\n",
       "0  {'neg': 0.212, 'neu': 0.727, 'pos': 0.061, 'co...  2017-10-01 20:00:00   \n",
       "1  {'neg': 0.117, 'neu': 0.749, 'pos': 0.133, 'co...  2017-10-02 08:35:09   \n",
       "2  {'neg': 0.284, 'neu': 0.659, 'pos': 0.057, 'co...  2017-10-02 07:47:00   \n",
       "3  {'neg': 0.255, 'neu': 0.687, 'pos': 0.059, 'co...  2017-10-02 10:51:55   \n",
       "4  {'neg': 0.154, 'neu': 0.789, 'pos': 0.057, 'co...  2017-10-02 10:35:10   \n",
       "\n",
       "                                               title themes  media_id  \\\n",
       "0  At Least 50 Killed as Gunman Opens Fire at Las...    NaN      1040   \n",
       "1  Gun-maker stocks surge after mass shooting in ...    NaN      1150   \n",
       "2  At Least 50 Dead, 400 Injured After Las Vegas ...    NaN     85364   \n",
       "3  Beware of These Hoaxes Being Spread About the ...    NaN     40362   \n",
       "4  Watch: President Trump speaks about Las Vegas ...    NaN     27512   \n",
       "\n",
       "                         media_url  \n",
       "0    http://realclearpolitics.com/  \n",
       "1             https://www.wsj.com/  \n",
       "2     http://www.newsy.com/#spider  \n",
       "3        http://www.time.com/time/  \n",
       "4  http://www.ibabuzz.com/insider/  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# My test\n",
    "vegas_df = af.combine_months_to_df('newspaper-text\\Vegas_Text-first-month.csv','newspaper-text\\Vegas_Text-second-month.csv','newspaper-text\\Vegas_Text-third-month.csv')\n",
    "\n",
    "vegas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a vocabulary of all the words in our data \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "'''\n",
    "We're using CountVectorizer to create a document-term matrix \n",
    "The paramters mean that we're only including words that appear in less than 80% of the document and in at least 2 documents\n",
    "Also removing stopwords\n",
    "'''\n",
    "count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "doc_term_matrix = count_vect.fit_transform(reviews_datasets['Text'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My test \n",
    "vegas_count_vect = CountVectorizer(max_df=0.8, min_df=2)\n",
    "vegas_doc_term_matrix =  vegas_count_vect.fit_transform(vegas_df['text'].values.astype('U'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<20000x14546 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 594703 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix\n",
    "'''\n",
    "This means that each of the 20000 documents is represented as a vector with \n",
    "1456 dimension - so our vocabulary has 14546 words'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1820x22025 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 563274 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# My test\n",
    "vegas_doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=5, random_state=42)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use LDA to create topics along the probability distribution for each word in our vocab for each topic\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "'''\n",
    "n_components specifics the number of topics that we want our text to be divided into\n",
    "random_state is the seed so you can replicate your results \n",
    "'''\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "LDA.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=5, random_state=42)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# My test \n",
    "vegas_LDA = LatentDirichletAllocation(n_components=5,random_state=42)\n",
    "vegas_LDA.fit(vegas_doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rivalries\n",
      "charity\n",
      "singapore\n",
      "consist\n",
      "stein\n",
      "worsham\n",
      "romanucci\n",
      "riskier\n",
      "publicists\n",
      "rough\n"
     ]
    }
   ],
   "source": [
    "# randomly fetch 10 words from the vocab\n",
    "import random\n",
    "\n",
    "for i in range(10):\n",
    "    random_id = random.randint(0,len(count_vect.get_feature_names()))\n",
    "    print(count_vect.get_feature_names()[random_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14106,  5892,  7088,  4290, 12596,  5771,  5187, 12888,  7498,\n",
       "       12921], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the 10 words with the highest probability for the first topic\n",
    "first_topic = LDA.components_[0]\n",
    "\"\"\" ^contains the probabilities of 14546 words for topic 1 \"\"\"\n",
    "top_topic_words = first_topic.argsort()[-10:]\n",
    "'''^ contains indexes of the 10 words with the highest probabilities '''\n",
    "top_topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parity\n",
      "dido\n",
      "espn\n",
      "compares\n",
      "mingled\n",
      "despite\n",
      "current\n",
      "motorist\n",
      "fall\n",
      "movies\n"
     ]
    }
   ],
   "source": [
    "# use the indexes to retreive the words from the count_vect object\n",
    "for i in top_topic_words:\n",
    "    print(count_vect.get_feature_names()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['parity', 'dido', 'espn', 'compares', 'mingled', 'despite', 'current', 'motorist', 'fall', 'movies']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['atlantis', 'bitcoin', 'ferocious', 'current', 'blackhawks', 'espn', 'dido', 'motorist', 'despite', 'fall']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['espn', 'compares', 'gulflive', 'mingled', 'maintains', 'parity', 'fall', 'erstwhile', 'increasing', 'atlantis']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['derpity', 'concocted', 'decades', 'increasing', 'fall', 'colossally', 'north', 'color', 'atlantis', 'dao']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['casualties', 'inaccurate', 'dido', 'fall', 'accord', 'despite', 'atlantis', 'increasing', 'castillo', 'bouts']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,topic in enumerate(LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #0:\n",
      "['team', 'night', 'season', 'could', 'use', 'vegas', 'new', 'game', 'said', 'first']\n",
      "\n",
      "\n",
      "Top 10 words for topic #1:\n",
      "['216', 'trump', 'mobile', 'arena', '27', 'new', 'round', 'first', 'fight', 'ufc']\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "['sunday', 'photo', 'oct', 'people', 'paddock', 'music', '2017', 'festival', 'las', 'vegas']\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "['also', 'news', 'time', 'like', 'one', 'would', 'trump', 'people', 'said', 'gun']\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "['killed', 'kelley', 'sunday', 'first', 'sutherland', 'springs', 'people', 'texas', 'church', 'said']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# My test\n",
    "for i,topic in enumerate(vegas_LDA.components_):\n",
    "    print(f'Top 10 words for topic #{i}:')\n",
    "    print([vegas_count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_values = LDA.transform(doc_term_matrix)\n",
    "'''output means that each of the documents have 5 columns where each column \n",
    "corresponds to the probabilitiy value of a particular topic'''\n",
    "topic_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \\\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...   \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...   \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...   \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...   \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...   \n",
       "\n",
       "   Topic  \n",
       "0      3  \n",
       "1      1  \n",
       "2      1  \n",
       "3      0  \n",
       "4      1  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_datasets['Topic'] = topic_values.argmax(axis=1)\n",
    "reviews_datasets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this guide: https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import All_Functions as af\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khahn\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\khahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vegas_df = af.combine_months_to_df('newspaper-text\\Vegas_Text-first-month.csv','newspaper-text\\Vegas_Text-second-month.csv','newspaper-text\\Vegas_Text-third-month.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string(text):\n",
    "    return text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = vegas_df['text'].map(split_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_text)\n",
    "\n",
    "# count = 0\n",
    "# for k, v in dictionary.iteritems():\n",
    "#     print(k, v)\n",
    "#     count += 1\n",
    "#     if count > 100:\n",
    "#         break\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow_doc_1800 = bow_corpus[1800]\n",
    "# for i in range(len(bow_doc_1800)):\n",
    "#     print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_1800[i][0], \n",
    "#                                                dictionary[bow_doc_1800[i][0]], \n",
    "# bow_doc_1800[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "# from pprint import pprint\n",
    "# for doc in corpus_tfidf:\n",
    "#     pprint(doc)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.008*\"gun\" + 0.006*\"use\" + 0.005*\"would\" + 0.005*\"2017\" + 0.004*\"trump\" + 0.004*\"''\" + 0.003*\"october\" + 0.003*\"may\" + 0.003*\"paddock\" + 0.003*\"new\"\n",
      "Topic: 1 \n",
      "Words: 0.005*\"fight\" + 0.005*\"two\" + 0.005*\"could\" + 0.004*\"night\" + 0.004*\"would\" + 0.004*\"new\" + 0.004*\"win\" + 0.004*\"t-mobile\" + 0.004*\"left\" + 0.004*\"killed\"\n",
      "Topic: 2 \n",
      "Words: 0.010*\"church\" + 0.009*\"texas\" + 0.008*\"''\" + 0.008*\"kelley\" + 0.006*\"gunman\" + 0.006*\"killed\" + 0.006*\"sunday\" + 0.005*\"gun\" + 0.005*\"2017\" + 0.004*\"springs\"\n",
      "Topic: 3 \n",
      "Words: 0.007*\"paddock\" + 0.006*\"''\" + 0.006*\"festival\" + 0.005*\"like\" + 0.005*\"2017\" + 0.005*\"police\" + 0.004*\"music\" + 0.004*\"told\" + 0.004*\"stephen\" + 0.004*\"gun\"\n",
      "Topic: 4 \n",
      "Words: 0.014*\"''\" + 0.008*\"trump\" + 0.005*\"gun\" + 0.005*\"new\" + 0.004*\"paddock\" + 0.004*\"would\" + 0.004*\"year\" + 0.004*\"president\" + 0.003*\"us\" + 0.003*\"like\"\n",
      "Topic: 5 \n",
      "Words: 0.012*\"gun\" + 0.009*\"''\" + 0.006*\"would\" + 0.005*\"new\" + 0.004*\"like\" + 0.004*\"trump\" + 0.004*\"guns\" + 0.004*\"--\" + 0.003*\"house\" + 0.003*\"two\"\n",
      "Topic: 6 \n",
      "Words: 0.007*\"''\" + 0.005*\"data\" + 0.004*\"provided\" + 0.004*\"killed\" + 0.004*\"2017\" + 0.004*\"gun\" + 0.004*\"police\" + 0.004*\"sunday\" + 0.004*\"festival\" + 0.004*\"factset\"\n",
      "Topic: 7 \n",
      "Words: 0.028*\"''\" + 0.016*\"church\" + 0.012*\"sutherland\" + 0.011*\"springs\" + 0.010*\"texas\" + 0.009*\"sunday\" + 0.008*\"2017\" + 0.007*\"baptist\" + 0.006*\"kelley\" + 0.005*\"gun\"\n",
      "Topic: 8 \n",
      "Words: 0.008*\"''\" + 0.006*\"church\" + 0.005*\"new\" + 0.004*\"trump\" + 0.004*\"texas\" + 0.004*\"gun\" + 0.004*\"killed\" + 0.003*\"would\" + 0.003*\"like\" + 0.003*\"shootings\"\n",
      "Topic: 9 \n",
      "Words: 0.021*\"2017\" + 0.020*\"festival\" + 0.017*\"music\" + 0.014*\"oct.\" + 0.014*\"sunday\" + 0.011*\"fire\" + 0.010*\"gunman\" + 0.010*\"ap\" + 0.010*\"2\" + 0.010*\"photo\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
