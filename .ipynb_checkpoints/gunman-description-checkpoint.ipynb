{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gunman Description \n",
    "To do:\n",
    "- isolate the search for adjectives describing the gunman to only the articles that are actually about the shooting at hand \n",
    "- compare the words across the different shootings/ partisan differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPORTs: packages, variables, files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import All_Functions as af \n",
    "from All_Functions import *\n",
    "import glob as glob \n",
    "from ast import literal_eval\n",
    "import nltk\n",
    "import collections\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def print_most_common(text):\n",
    "    counter = Counter(text)\n",
    "    most = counter.most_common()\n",
    "    print(most[:10])\n",
    "    \n",
    "# FUNCTIONS\n",
    "def get_relevant_articles(shooting_list):\n",
    "    \"\"\" INPUT: a list of lists where each sublist is an article w/ corresponding data from a given shooting \n",
    "        ASSUMPTIONS: We're asserting that if an article contains more than 3 keywords, the article is *about* the shooting \n",
    "        OUTPUT: a list of the shooting we're talking about, the # of articles about the shooting, the total # of articles in the dataset, the avg word count of the relevant articles \n",
    "    \"\"\"\n",
    "    count = [0 for x in range(len(shooting_list))] # set up an index to keep track of when/ how many keywords appear \n",
    "    kw_appearance = [[] for x in range(len(shooting_list))]\n",
    "    location = shooting_list[1][-2] # the \"name\" of the shooting which links to the dictionary\n",
    "\n",
    "    for i in range(len(shooting_list)):\n",
    "        lower = shooting_list[i][2].lower() \n",
    "        shooting_list[i][2] = lower\n",
    "         \n",
    "        for word in nltk.word_tokenize(shooting_list[i][2]): # for single words in the dict\n",
    "            if word in shooting_keywords[location]:\n",
    "                count[i] +=1\n",
    "                kw_appearance[i].append(word)\n",
    "        for word in af.extract_ngrams(nltk.word_tokenize(shooting_list[i][2]), 2): # for bigrams in the dict\n",
    "            if word in shooting_keywords[location]:\n",
    "                count[i] += 1\n",
    "                kw_appearance[i].append(word)\n",
    "        for word in af.extract_ngrams(nltk.word_tokenize(shooting_list[i][2]), 3): # for trigrams in the dict\n",
    "            if word in shooting_keywords[location]:\n",
    "                count[i] += 1\n",
    "                kw_appearance[i].append(word)\n",
    "#     for index in range(len(shooting_list)):\n",
    "#         shooting_list[i].append(count[i])\n",
    "    relevant_articles = []\n",
    "    for i in range(len(shooting_list)):\n",
    "        if count[i] > 3: # Here is where we enter the assumption of 3\n",
    "            relevant_articles.append(shooting_list[i])\n",
    "            \n",
    "    # CAN Also return kw_appearance if you want to see the words that are pinging\n",
    "    return relevant_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the unclean text files\n",
    "all_files = [x for x in glob.glob('newspaper-text/unclean-sentencesplit' + \"/*.csv\")]\n",
    "all_text = af.import_text_data(all_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ACTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get just the text \n",
    "just_text = []\n",
    "for i in range(len(all_text)):\n",
    "    for j in range(1,len(all_text[i])):\n",
    "        mini_text = []\n",
    "        literal = literal_eval(all_text[i][j][2]) # for some reason the lists are imported as strings so this fixes that \n",
    "        tokenized = [nltk.word_tokenize(x) for x in literal]\n",
    "#         lower= af.remove_capitalization(tokenized)\n",
    "#         for x in literal:\n",
    "#             tokenized = nltk.word_tokenize(x)\n",
    "#             mini_text.append(x) # now we have just one list of every string from every shooting article\n",
    "        for x in tokenized:\n",
    "            lower= af.remove_capitalization(x)\n",
    "            just_text.append(lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find synonyms of the word \"gunman\"\n",
    "Using Word2Vec, find synonyms of the word \"gunman\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences=just_text, window=5, min_count=1, workers=4)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec.model\")\n",
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in word_vectors.key_to_index:\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model.wv['gunman']  # get numpy vector of a word\n",
    "sims = model.wv.most_similar('gunman', topn=30)  # get other similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shooter', 0.8254361748695374),\n",
       " ('suspect', 0.7077057361602783),\n",
       " ('gunmen', 0.6907996535301208),\n",
       " ('attacker', 0.6741224527359009),\n",
       " ('man', 0.6522361040115356),\n",
       " ('paddock', 0.6377201676368713),\n",
       " ('assailant', 0.6344540119171143),\n",
       " ('killer', 0.629865288734436),\n",
       " ('kelley', 0.5895920395851135),\n",
       " ('perpetrator', 0.5850415229797363),\n",
       " ('rampage', 0.5806468725204468),\n",
       " ('craddock', 0.5793517827987671),\n",
       " ('ator', 0.5637785196304321),\n",
       " ('massacre', 0.5572240352630615),\n",
       " ('attack', 0.5498966574668884),\n",
       " ('suspects', 0.5496650338172913),\n",
       " ('cassidy', 0.5487622618675232),\n",
       " ('xaver', 0.5304782390594482),\n",
       " ('sniper', 0.5276758074760437),\n",
       " ('mateen', 0.5257592797279358),\n",
       " ('shooting', 0.525025486946106),\n",
       " ('atchison', 0.5226256847381592),\n",
       " ('assailants', 0.5135753154754639),\n",
       " ('wooden-framed', 0.5122582912445068),\n",
       " ('gunfire', 0.5016877055168152),\n",
       " ('bar', 0.5012877583503723),\n",
       " ('hotel', 0.5009043216705322),\n",
       " ('roof', 0.49991390109062195),\n",
       " ('farook', 0.49825799465179443),\n",
       " ('shoplifter', 0.4953761696815491)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "gunman_words = ['gunman','shooter','suspect','gunmen','attacker','assailant',\n",
    "                'killer','perpetrator','suspects']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up a POS tagger to identify the words that describe \"gunman\" and its synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\khahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\khahn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "nltk.download('treebank')\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "from nltk.tag.brill_trainer import BrillTaggerTrainer\n",
    "from nltk.tag.brill import brill24\n",
    "from nltk.tag import brill, brill_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cutoff = 20000\n",
    "brown_sents_train = brown.tagged_sents()[0:n_cutoff] # training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_tagger = UnigramTagger(brown_sents_train)\n",
    "bi_tagger_backoff = BigramTagger(brown_sents_train, backoff=uni_tagger)\n",
    "bi_tagger = BigramTagger(brown_sents_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Add a timer to this next part: it takes ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with the backoff\n",
    "templates = nltk.tag.brill.brill24()\n",
    "brill_tagger = brill_trainer.BrillTaggerTrainer(bi_tagger, templates)\n",
    "trainer = brill_tagger.train(brown_sents_train)\n",
    "brill_tagger_backoff = brill_trainer.BrillTaggerTrainer(bi_tagger_backoff, templates)\n",
    "trainer_backoff = brill_tagger_backoff.train(brown_sents_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_backoff.tag(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Now I want to be able to connect the results to a specific shooting, so I need to build exactly the same database except with location attached\"\"\"\n",
    "\"\"\"The issue is many of the article from a specific shooting aren't really about that shooting\"\"\"\n",
    "# Get just the text \n",
    "text_loc = []\n",
    "for i in range(len(all_text)):\n",
    "    for j in range(1,len(all_text[i])):\n",
    "        literal = literal_eval(all_text[i][j][2]) # for some reason the lists are imported as strings so this fixes that \n",
    "        tokenized = [nltk.word_tokenize(x) for x in literal]\n",
    "        location = all_text[i][j][-2]\n",
    "        for x in tokenized:\n",
    "            lower= af.remove_capitalization(x)\n",
    "            text_loc.append([lower, [location]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "gunman_sentences =[]\n",
    "for i in range(len(text_loc)):\n",
    "    for word in text_loc[i][0]:\n",
    "        if word in gunman_words:\n",
    "            gunman_sentences.append(text_loc[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gunman_sentences)):\n",
    "    tags = trainer_backoff.tag(gunman_sentences[i][0])\n",
    "    gunman_sentences[i].append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more sophisticated version with \n",
    "adjs = ['JJ','JJS','JJR']\n",
    "shootings = []\n",
    "gunman_adjs = [[] for x in range(10)]\n",
    "for i in range(len(gunman_sentences)):\n",
    "    # First, get the location & index of the location in \"shootings\"\n",
    "    location = gunman_sentences[i][1]\n",
    "    if location not in shootings:\n",
    "        shootings.append(location)\n",
    "    for j in range(len(shootings)):\n",
    "        if location == shootings[j]:\n",
    "            index = j\n",
    "    # then get all the adjectives from the sentences\n",
    "    for ii in range(len(gunman_sentences[i][-1])): \n",
    "        if ii > 0 and gunman_sentences[i][-1][ii][0] in gunman_words:\n",
    "            if gunman_sentences[i][-1][ii-1][1] in adjs:\n",
    "#                 print( gunman_sentences[i][-1][ii-1][0],gunman_sentences[i][-1][ii][0] )\n",
    "                gunman_adjs[index].append(gunman_sentences[i][-1][ii-1][0])\n",
    "            if gunman_sentences[i][-1][ii-2][1] in adjs:\n",
    "#                 print(gunman_sentences[i][-1][ii-2][0], gunman_sentences[i][-1][ii-1][0],gunman_sentences[i][-1][ii][0] )\n",
    "#                 print(gunman_sentences[i][-1][ii-2][1], gunman_sentences[i][-1][ii-1][1],gunman_sentences[i][-1][ii][1] )\n",
    "                gunman_adjs[index].append(gunman_sentences[i][-1][ii-2][0])\n",
    "                gunman_adjs[index].append(gunman_sentences[i][-1][ii-1][0])\n",
    "            if gunman_sentences[i][-1][ii+1][1] in adjs:\n",
    "#                 print(gunman_sentences[i][-1][ii][0], gunman_sentences[i][-1][ii+1][0] )\n",
    "#                 print(gunman_sentences[i][-1][ii-2][1], gunman_sentences[i][-1][ii-1][1],gunman_sentences[i][-1][ii][1] )\n",
    "                gunman_adjs[index].append(gunman_sentences[i][-1][ii+1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that describe the gunman in each shooting:\n",
      "['Bogue'] 99\n",
      "['Boulder'] 520\n",
      "['DC'] 147\n",
      "['Houston'] 146\n",
      "['Odessa'] 333\n",
      "['Pittsburgh'] 265\n",
      "['Plano'] 1242\n",
      "['SanBernadino'] 360\n",
      "['Vegas'] 1244\n",
      "['VirginiaBeach'] 150\n"
     ]
    }
   ],
   "source": [
    "print('Number of words that describe the gunman in each shooting:')\n",
    "for index in range(len(gunman_adjs)):\n",
    "    print(shootings[index], len(gunman_adjs[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words to describe each shooter:\n",
      "['Bogue']\n",
      "[('active', 34), ('lone', 9), ('to', 6), ('shooting', 5), ('white', 4), ('reasonable', 4), ('multiple', 4), ('the', 3), ('possible', 2), ('correct', 2)]\n",
      "['Boulder']\n",
      "[('active', 195), ('lone', 47), ('teenage', 36), ('21-year-old', 32), ('dead', 14), ('white', 13), ('possible', 13), ('middle-aged', 10), ('old', 7), ('the', 7)]\n",
      "['DC']\n",
      "[('lone', 19), ('possible', 18), ('dead', 17), ('sandy', 12), ('hook', 12), ('second', 9), ('potential', 8), ('crazy', 8), ('active', 6), ('elementary', 3)]\n",
      "['Houston']\n",
      "[('active', 52), ('26-year-old', 10), ('lone', 8), ('late', 7), ('would-be', 5), ('dead', 5), ('young', 4), ('such', 4), ('responsible', 3), ('the', 3)]\n",
      "['Odessa']\n",
      "[('active', 110), ('lone', 23), ('white', 14), ('21-year-old', 10), ('unknown', 10), ('mass', 9), ('recent', 9), ('supremacist', 9), ('potential', 8), ('dead', 7)]\n",
      "['Pittsburgh']\n",
      "[('active', 115), ('lone', 24), ('dead', 16), ('possible', 11), ('shooting', 11), ('white', 9), ('mass', 6), ('initial', 5), ('outspoken', 4), ('multiple', 3)]\n",
      "['Plano']\n",
      "[('active', 680), ('lone', 195), ('sole', 52), ('dead', 41), ('white', 17), ('26-year-old', 14), ('possible', 11), ('real', 9), ('local', 9), ('responsible', 9)]\n",
      "['SanBernadino']\n",
      "[('active', 135), ('female', 36), ('dead', 28), ('possible', 16), ('lone', 15), ('good', 11), ('penalty', 11), ('mass', 10), ('white', 10), ('would-be', 7)]\n",
      "['Vegas']\n",
      "[('active', 676), ('lone', 193), ('sole', 50), ('dead', 45), ('white', 16), ('26-year-old', 14), ('real', 9), ('local', 9), ('possible', 9), ('responsible', 9)]\n",
      "['VirginiaBeach']\n",
      "[('active', 69), ('school', 10), ('high', 9), ('likely', 5), ('lone', 5), ('the', 4), ('possible', 4), ('second', 4), ('dead', 3), ('hasty', 3)]\n"
     ]
    }
   ],
   "source": [
    "print('Most common words to describe each shooter:')\n",
    "\n",
    "for index in range(len(gunman_adjs)):\n",
    "    print(shootings[index])\n",
    "    print_most_common(gunman_adjs[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat this process but just for the relevant articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "shooting_keywords = {'Plano':[\"spencer hight\",\"dallas cowboys\",\"meredith hight\",\"dallas\",\"plano\",\"sunday\",\"caleb edwards\",\"deffner\",\"rushin\"],\n",
    "                     'Pittsburgh':[\"pittsburgh\",\"synagogue\",\"bowers\",\"tree of life\",\"squirrel hill\",\"jewish\",\"anti-semitism\",\"jews\"],\n",
    "                     'Vegas':[\"paddock\",\"mandalay bay hotel\",\"route 91 harvest\",\"las vegas\",\"aldean\",\"concert\",\"mesquite\",\"hotel\",\"lombardo\"],\n",
    "                     'SanBernadino':['syed','rizwan','farook','tashfeen','malik','SUV',\"inland regional center\",\"san bernardino\",\"redlands\",\"christmas party\",\"public health department\",\"bomb\"],\n",
    "                     'Houston':['david','conley','harris county','ex-girlfriend','houston','saturday','valerie jackson','window','black','arrested'],\n",
    "                     'Odessa':['saturday','midland','odessa','seth','aaron','ator','west texas','traffic stop','white van','movie theater','random','white'],\n",
    "                     'Bogue':['willie','cory','godbolt','arrested','lincoln county','bogue chitto','brookhaven','barbara mitchell'],\n",
    "                     'DC':['washington navy yard','aaron alexis','monday','contractor','12 people'],\n",
    "                     'Boulder':['king soopers','boulder','ahmad al','aliwi','al-issa','arrested','9mm handgun','table mesa drive','eric talley','boulder police','monday','in custody'],\n",
    "                     'VirginiaBeach':['virginia beach','dewayne','craddock','employee','nettleton','.45-caliber','engineer','municipal']\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_articles = [get_relevant_articles(x) for x in all_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(rel_art[1][2].split())\n",
    "# nltk.word_tokenize(rel_art[1][2])\n",
    "# af.extract_ngrams(nltk.word_tokenize(rel_art[5][2]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-300-52b4b05de429>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrel_art\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mrel_art\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_relevant_articles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_text\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrel_art\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Keywords: {kw[i]}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "rel_art, kw= []\n",
    "rel_art, kw = get_relevant_articles(all_text[-1])\n",
    "for i in range(len(rel_art)):\n",
    "    if len(kw[i]) > 0:\n",
    "        print(f\"Keywords: {kw[i]}\")\n",
    "        print(f\"Index: {i}\")\n",
    "        print(f\"Title: {rel_art[i][4]}\")\n",
    "        print(f\"Url: {rel_art[i][1]}\")\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "556\n",
      "57\n",
      "74\n",
      "306\n",
      "449\n",
      "363\n",
      "477\n",
      "1840\n",
      "141\n"
     ]
    }
   ],
   "source": [
    "for x in relevant_articles:\n",
    "    print(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get just the text \n",
    "text_loc_v2 = []\n",
    "for i in range(len(relevant_articles)):\n",
    "    for j in range(1,len(relevant_articles[i])):\n",
    "        literal = literal_eval(relevant_articles[i][j][2]) # for some reason the lists are imported as strings so this fixes that \n",
    "        tokenized = [nltk.word_tokenize(x) for x in literal]\n",
    "        location = all_text[i][j][-2]\n",
    "        for x in tokenized:\n",
    "            lower= af.remove_capitalization(x)\n",
    "            text_loc_v2.append([lower, [location]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "gunman_sentences = []\n",
    "for i in range(len(text_loc_v2)):\n",
    "    for word in text_loc_v2[i][0]:\n",
    "        if word in gunman_words:\n",
    "            gunman_sentences.append(text_loc_v2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(gunman_sentences)):\n",
    "    tags = trainer_backoff.tag(gunman_sentences[i][0])\n",
    "    gunman_sentences[i].append(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more sophisticated version with \n",
    "adjs = ['JJ','JJS','JJR']\n",
    "shootings = []\n",
    "gunman_adjs = [[] for x in range(10)]\n",
    "for i in range(len(gunman_sentences)):\n",
    "    # First, get the location & index of the location in \"shootings\"\n",
    "    location = gunman_sentences[i][1]\n",
    "    if location not in shootings:\n",
    "        shootings.append(location)\n",
    "    for j in range(len(shootings)):\n",
    "        if location == shootings[j]:\n",
    "            index = j\n",
    "    # then get all the adjectives from the sentences\n",
    "    for ii in range(len(gunman_sentences[i][-1])): \n",
    "        if ii > 0 and gunman_sentences[i][-1][ii][0] in gunman_words:\n",
    "            if gunman_sentences[i][-1][ii-1][1] in adjs:\n",
    "#                 print( gunman_sentences[i][-1][ii-1][0],gunman_sentences[i][-1][ii][0] )\n",
    "                gunman_adjs[index].append(gunman_sentences[i][-1][ii-1][0])\n",
    "            if gunman_sentences[i][-1][ii-2][1] in adjs:\n",
    "#                 print(gunman_sentences[i][-1][ii-2][0], gunman_sentences[i][-1][ii-1][0],gunman_sentences[i][-1][ii][0] )\n",
    "#                 print(gunman_sentences[i][-1][ii-2][1], gunman_sentences[i][-1][ii-1][1],gunman_sentences[i][-1][ii][1] )\n",
    "                gunman_adjs[index].append(gunman_sentences[i][-1][ii-2][0])\n",
    "                gunman_adjs[index].append(gunman_sentences[i][-1][ii-1][0])\n",
    "            if gunman_sentences[i][-1][ii+1][1] in adjs:\n",
    "#                 print(gunman_sentences[i][-1][ii][0], gunman_sentences[i][-1][ii+1][0] )\n",
    "#                 print(gunman_sentences[i][-1][ii-2][1], gunman_sentences[i][-1][ii-1][1],gunman_sentences[i][-1][ii][1] )\n",
    "                gunman_adjs[index].append(gunman_sentences[i][-1][ii+1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words that describe the gunman in each shooting:\n",
      "['Bogue'] 0\n",
      "['Boulder'] 193\n",
      "['DC'] 74\n",
      "['Houston'] 20\n",
      "['Odessa'] 130\n",
      "['Pittsburgh'] 82\n",
      "['Plano'] 684\n",
      "['SanBernadino'] 194\n",
      "['Vegas'] 970\n",
      "['VirginiaBeach'] 38\n"
     ]
    }
   ],
   "source": [
    "print('Number of words that describe the gunman in each shooting:')\n",
    "for index in range(len(gunman_adjs)):\n",
    "    print(shootings[index], len(gunman_adjs[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words to describe each shooter:\n",
      "['Bogue']\n",
      "[]\n",
      "['Boulder']\n",
      "[('active', 79), ('21-year-old', 25), ('lone', 13), ('middle-aged', 10), ('prime', 6), ('sure', 5), ('white', 4), ('this', 4), ('possible', 4), ('unknown', 3)]\n",
      "['DC']\n",
      "[('lone', 17), ('possible', 16), ('dead', 14), ('second', 9), ('potential', 7), ('additional', 2), ('well-armed', 2), ('active', 2), ('nationwide', 1), ('multiple', 1)]\n",
      "['Houston']\n",
      "[('late', 7), ('such', 4), ('possible', 2), ('would-be', 1), ('active', 1), ('potential', 1), ('medical', 1), ('layman', 1), ('glad', 1), ('the', 1)]\n",
      "['Odessa']\n",
      "[('active', 30), ('lone', 13), ('white', 11), ('supremacist', 8), ('21-year-old', 6), ('mass', 5), ('potential', 4), ('recent', 4), ('years', 4), ('new', 4)]\n",
      "['Pittsburgh']\n",
      "[('active', 38), ('possible', 11), ('shooting', 11), ('lone', 7), ('outspoken', 4), ('mass', 3), ('likely', 1), ('apparent', 1), ('notorious', 1), ('responsible', 1)]\n",
      "['Plano']\n",
      "[('active', 497), ('lone', 82), ('sole', 46), ('dead', 7), ('26-year-old', 6), ('local', 4), ('time.the', 4), ('recreational', 4), ('open', 3), ('main', 3)]\n",
      "['SanBernadino']\n",
      "[('active', 53), ('female', 31), ('dead', 24), ('possible', 14), ('good', 11), ('penalty', 11), ('mass', 7), ('white', 5), ('would-be', 4), ('lone', 4)]\n",
      "['Vegas']\n",
      "[('active', 599), ('lone', 151), ('sole', 50), ('dead', 29), ('local', 9), ('white', 9), ('wolf', 7), ('real', 6), ('extra', 5), ('open', 5)]\n",
      "['VirginiaBeach']\n",
      "[('active', 20), ('likely', 4), ('hasty', 2), ('and', 2), ('mass', 1), ('dead', 1), ('alien', 1), ('serial', 1), ('alive', 1), ('revealing', 1)]\n"
     ]
    }
   ],
   "source": [
    "print('Most common words to describe each shooter:')\n",
    "\n",
    "for index in range(len(gunman_adjs)):\n",
    "    print(shootings[index])\n",
    "    print_most_common(gunman_adjs[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands on Approach\n",
    "Okay now gonna try a more hands on approach for Bogue bc there have got to be sentences that describe the shooter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "bogue = all_text[0]\n",
    "for i in range(len(bogue[:82])):\n",
    "    tokenized = nltk.word_tokenize(bogue[i][2])\n",
    "    for j in range(len(tokenized)):\n",
    "        if tokenized[j] == 'godbolt':\n",
    "            print(tokenized[j-2],tokenized[j-1],tokenized[j],tokenized[j+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cant find evidence of a single article about the Bogue shooting...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
